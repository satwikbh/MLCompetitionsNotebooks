{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextDataAugmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j60olXR2T6LY",
        "aO41EbVXTze4",
        "fJnSmG5EzvXx",
        "uIUO9r5HzSua",
        "ipHgtoNLzVfP",
        "iGEjzIsI0H4G"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeQQBulaRrH6",
        "colab_type": "text"
      },
      "source": [
        "# AICrowd - Hate Speech Detection from tweets [Link](https://www.aicrowd.com/challenges/htspc-hate-speech-classification)\n",
        "\n",
        "1. Load the data & Load word2vec model\n",
        "2. Pre-process the data \\\\\n",
        "    a. Basic tokenizing, stopwords removal \\\\\n",
        "    b. For each tweet, get sentiment analysis \\\\\n",
        "    c. For each tweet, do POS tagging and pick the adjectives & adverbs. \\\\\n",
        "3. Once we have the sentiment analysis and adjective words for each tweet: \\\\\n",
        "    a. Load \"n\" similar words for the adjective(s). \\\\\n",
        "    b. Replace the adjective with the similar word to create more sentences. \\\\\n",
        "    c. The label for the sentences will be the same as their parent sentences. \\\\\n",
        "    d. Check the sentiment of the new sentences, this will be label_sent. \\\\\n",
        "    e. In case of conflict between parent and the label_sent, give priority to label_sent.\n",
        "4. Now that we have increased the dataset size, we proceed to perform classification using ML Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j60olXR2T6LY",
        "colab_type": "text"
      },
      "source": [
        "## Google Drive & Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BceT_HUARhYP",
        "colab_type": "code",
        "outputId": "4e331877-c5aa-42ba-b946-466c90a10aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnMnKfNnUOwi",
        "colab_type": "code",
        "outputId": "28ca2a3e-59d0-4910-a77d-d1b3b580b7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "!pip install vaderSentiment textstat\n",
        "!pip install textblob"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/a3/1218a3b5651dbcba1699101c84e5c84c36cbba360d9dbf29f2ff18482982/vaderSentiment-3.3.1-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hCollecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/af/0623a6e3adbcfda0be827664eacab5e02cd0a08d36f00013cb53784917a9/textstat-0.6.2-py3-none-any.whl (102kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.2MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.2MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment, pyphen, textstat\n",
            "Successfully installed pyphen-0.9.5 textstat-0.6.2 vaderSentiment-3.3.1\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktPiDB-YT5V-",
        "colab_type": "code",
        "outputId": "02e17715-6e18-4325-e631-d973755c38b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
        "from textblob import TextBlob\n",
        "\n",
        "from time import time\n",
        "from collections import Counter, defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, InputLayer, GlobalMaxPooling1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks.callbacks import Callback, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO41EbVXTze4",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the Data & word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDUplQTATzF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"/gdrive/My Drive/Dataset/AICrowd/HateSpeechDetection/1fe720be-90e4-4e06-9b52-9de93e0ea937_train.csv\")\n",
        "test_data = pd.read_csv(\"/gdrive/My Drive/Dataset/AICrowd/HateSpeechDetection/f6eb0bd7-6063-4e50-baa0-111feda638fb_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ0mdYdBXEZR",
        "colab_type": "code",
        "outputId": "d3277a61-e20f-4294-d2ef-4a321348c9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "word2vec_model_path = \"/gdrive/My Drive/Dataset/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "w = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGau_PRtXOI3",
        "colab_type": "code",
        "outputId": "d1bdbc67-467c-4ece-e1a3-fb9ab008ea26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "w.most_similar(positive=[\"fuck\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fucking', 0.8137822151184082),\n",
              " ('f_*_ck', 0.801154613494873),\n",
              " ('f_**_k', 0.7815893888473511),\n",
              " ('shit', 0.7604621648788452),\n",
              " ('fucked', 0.7501130104064941),\n",
              " ('fuckin', 0.7309141755104065),\n",
              " ('f_***', 0.7172753810882568),\n",
              " ('f_ck', 0.7121477127075195),\n",
              " ('f_---', 0.7099311351776123),\n",
              " ('Fuck', 0.7066987752914429)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4kVLzpgzadl",
        "colab_type": "text"
      },
      "source": [
        "## 2. Pre-process the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJnSmG5EzvXx",
        "colab_type": "text"
      },
      "source": [
        "### Basic Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzGtp7ZmHDvt",
        "colab_type": "code",
        "outputId": "46d0cc88-aedb-48cd-9c37-39c91c4601e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def clean_text(line):\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "    combined_pat = r'|'.join((pat1, pat2))\n",
        "    \n",
        "    soup = BeautifulSoup(line, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    split = lower_case.split(\" \")\n",
        "    return \" \".join(list(filter(None, split))).strip()\n",
        "\n",
        "def get_indices(x): return np.argmax(x)\n",
        "\n",
        "train_data[\"clean_text\"] = train_data.text.apply(lambda x: clean_text(x))\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         clean_text\n",
              "0  @realDonaldTrump This is one of the worst time...  ...  this is one of the worst times to be american ...\n",
              "1  How about the crowd in Oval in today's #AUSvIN...  ...  how about the crowd in oval in today s ausvind...\n",
              "2  @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...  biden his son hunter took advantage of their p...\n",
              "3  #etsy shop: Benedict Donald so called presiden...  ...  etsy shop benedict donald so called president ...\n",
              "4  @realDonaldTrump Good build a wall around Arka...  ...  good build a wall around arkansas fucktrump fu...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIUO9r5HzSua",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg5P1o4GJtsd",
        "colab_type": "code",
        "outputId": "77679b41-dead-4920-a9b4-0a625b970101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def get_sentiment_1(text):\n",
        "    analysis = TextBlob(text)\n",
        "    val = analysis.sentiment.polarity\n",
        "    if val > 0:\n",
        "        return 1\n",
        "    elif val < 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "train_data[\"sentiment_1\"] = train_data[\"clean_text\"].apply(lambda x: get_sentiment_1(x))\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentiment_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...  sentiment_1\n",
              "0  @realDonaldTrump This is one of the worst time...  ...            0\n",
              "1  How about the crowd in Oval in today's #AUSvIN...  ...           -1\n",
              "2  @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...            0\n",
              "3  #etsy shop: Benedict Donald so called presiden...  ...           -1\n",
              "4  @realDonaldTrump Good build a wall around Arka...  ...            1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtTV7ypkbPJR",
        "colab_type": "code",
        "outputId": "8227ab51-606d-4e5d-ac4d-b4af35845974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "sentiment_analyzer = VS()\n",
        "\n",
        "def get_sentiment_2(text, sentiment_analyzer):\n",
        "    sentiment = sentiment_analyzer.polarity_scores(text)\n",
        "    negative, neutral, positive = sentiment[\"neg\"], sentiment[\"neu\"], sentiment[\"pos\"]\n",
        "    if negative > neutral and negative > positive:\n",
        "        return 0\n",
        "    elif positive > negative and positive > neutral:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1 \n",
        "\n",
        "\n",
        "# for index, row in train_data.head(20).iterrows():\n",
        "#     text, act_label, clean_text = row[0], row[1], row[2]\n",
        "#     sentiment = sentiment_analyzer.polarity_scores(text)\n",
        "#     clean_sentiment = sentiment_analyzer.polarity_scores(clean_text)\n",
        "#     print(index, act_label, [sentiment[\"neg\"], sentiment[\"pos\"]], [clean_sentiment[\"neg\"], clean_sentiment[\"pos\"]])\n",
        "\n",
        "train_data[\"sentiment_2\"] = train_data[\"clean_text\"].apply(lambda x: get_sentiment_2(x, sentiment_analyzer))\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentiment_1</th>\n",
              "      <th>sentiment_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...  sentiment_2\n",
              "0  @realDonaldTrump This is one of the worst time...  ...           -1\n",
              "1  How about the crowd in Oval in today's #AUSvIN...  ...           -1\n",
              "2  @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...           -1\n",
              "3  #etsy shop: Benedict Donald so called presiden...  ...           -1\n",
              "4  @realDonaldTrump Good build a wall around Arka...  ...           -1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipHgtoNLzVfP",
        "colab_type": "text"
      },
      "source": [
        "### POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvYHdYB0BF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pos_tags(txt, stopwords, target_forms):\n",
        "    tokenized = sent_tokenize(txt) \n",
        "    words = list()\n",
        "    for i in tokenized:\n",
        "        words_list = nltk.word_tokenize(i) \n",
        "        words_list = [w for w in words_list if not w in stopwords] \n",
        "        tagged = nltk.pos_tag(words_list) \n",
        "        words += [word for word, pos_tag in tagged if pos_tag in target_forms]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXi1VL1u01yZ",
        "colab_type": "code",
        "outputId": "1038617c-7995-4230-eb90-c1e1fe4d8587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "target_forms = [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
        "stopwords.extend(other_exclusions)\n",
        "\n",
        "train_data[\"target_words\"] = train_data[\"clean_text\"].apply(lambda x: get_pos_tags(x, stopwords, target_forms))\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentiment_1</th>\n",
              "      <th>sentiment_2</th>\n",
              "      <th>target_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[worst, american, serious, sure, wish, happy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[indian]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[biden, ukraine, dead]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[short, streetwear]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[good]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                       target_words\n",
              "0  @realDonaldTrump This is one of the worst time...  ...  [worst, american, serious, sure, wish, happy, ...\n",
              "1  How about the crowd in Oval in today's #AUSvIN...  ...                                           [indian]\n",
              "2  @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...                             [biden, ukraine, dead]\n",
              "3  #etsy shop: Benedict Donald so called presiden...  ...                                [short, streetwear]\n",
              "4  @realDonaldTrump Good build a wall around Arka...  ...                                             [good]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGEjzIsI0H4G",
        "colab_type": "text"
      },
      "source": [
        "## 3. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMLKkXyV32hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_k_words(model, word, k):\n",
        "    try:\n",
        "        similar = model.most_similar(positive=[word], topn=k)\n",
        "    except KeyError as e:\n",
        "        similar = []\n",
        "    return similar\n",
        "\n",
        "def replace_and_create(text, words, model, k=2):\n",
        "    try:\n",
        "        augmented_text = list()\n",
        "        for word in words:\n",
        "            similar_words = get_top_k_words(model, word, k)\n",
        "            if len(similar_words) != 0:\n",
        "                for each_word in similar_words:\n",
        "                    augmented_text.append(text.replace(word, each_word[0]))\n",
        "    except Exception as e:\n",
        "        print(e, words, similar_words)\n",
        "    return augmented_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VvIhn2n0K-7",
        "colab_type": "code",
        "outputId": "7335be72-8901-4071-d15b-11fbe40d2b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "top_k = 2\n",
        "start_time = 0\n",
        "column = list()\n",
        "\n",
        "print(\"Total no of rows : {}\".format(train_data.shape[0]))\n",
        "\n",
        "for index, row in train_data.iterrows():\n",
        "    if index % 100 == 0:\n",
        "        print(index, time() - start_time)\n",
        "        start_time = time()\n",
        "    clean_text, target_words = row[2], row[5]\n",
        "    v = replace_and_create(text=clean_text, words=target_words, model=w, k=top_k)\n",
        "    column.append(v)\n",
        "\n",
        "train_data[\"augmented_texts\"] = column\n",
        "train_data.to_csv(\"/gdrive/My Drive/Dataset/AICrowd/HateSpeechDetection/Augmented_data.csv\", index=False)\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no of rows : 5266\n",
            "0 1588249769.7477138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100 63.0937922000885\n",
            "200 61.92446231842041\n",
            "300 69.8088641166687\n",
            "400 69.11589050292969\n",
            "500 76.36979961395264\n",
            "600 57.246421575546265\n",
            "700 62.71471452713013\n",
            "800 53.33625388145447\n",
            "900 69.03457260131836\n",
            "1000 63.53877139091492\n",
            "1100 75.59937620162964\n",
            "1200 76.38743925094604\n",
            "1300 66.55491161346436\n",
            "1400 73.54179501533508\n",
            "1500 72.98031759262085\n",
            "1600 55.562992572784424\n",
            "1700 69.28310656547546\n",
            "1800 65.42160844802856\n",
            "1900 66.79487609863281\n",
            "2000 64.0601601600647\n",
            "2100 65.84169673919678\n",
            "2200 61.25098180770874\n",
            "2300 78.31003427505493\n",
            "2400 65.72362446784973\n",
            "2500 57.83689641952515\n",
            "2600 55.99736022949219\n",
            "2700 60.053537368774414\n",
            "2800 65.96400260925293\n",
            "2900 65.80471229553223\n",
            "3000 71.54007625579834\n",
            "3100 75.94544100761414\n",
            "3200 68.97627902030945\n",
            "3300 62.50282025337219\n",
            "3400 68.41259670257568\n",
            "3500 66.74055171012878\n",
            "3600 56.97658395767212\n",
            "3700 65.18144488334656\n",
            "3800 63.735482692718506\n",
            "3900 73.65312361717224\n",
            "4000 63.76209211349487\n",
            "4100 72.79196238517761\n",
            "4200 67.43505477905273\n",
            "4300 76.05166125297546\n",
            "4400 58.262911796569824\n",
            "4500 77.29177904129028\n",
            "4600 76.38693952560425\n",
            "4700 74.60694599151611\n",
            "4800 80.00590777397156\n",
            "4900 70.75604581832886\n",
            "5000 57.531490325927734\n",
            "5100 65.78575539588928\n",
            "5200 72.59207677841187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentiment_1</th>\n",
              "      <th>sentiment_2</th>\n",
              "      <th>target_words</th>\n",
              "      <th>augmented_texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[worst, american, serious, sure, wish, happy, ...</td>\n",
              "      <td>[this is one of the Worst times to be american...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[indian]</td>\n",
              "      <td>[how about the crowd in oval in today s ausvin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[biden, ukraine, dead]</td>\n",
              "      <td>[john_mccain his son hunter took advantage of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[short, streetwear]</td>\n",
              "      <td>[etsy shop benedict donald so called president...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[good]</td>\n",
              "      <td>[great build a wall around arkansas fucktrump ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                    augmented_texts\n",
              "0  @realDonaldTrump This is one of the worst time...  ...  [this is one of the Worst times to be american...\n",
              "1  How about the crowd in Oval in today's #AUSvIN...  ...  [how about the crowd in oval in today s ausvin...\n",
              "2  @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...  [john_mccain his son hunter took advantage of ...\n",
              "3  #etsy shop: Benedict Donald so called presiden...  ...  [etsy shop benedict donald so called president...\n",
              "4  @realDonaldTrump Good build a wall around Arka...  ...  [great build a wall around arkansas fucktrump ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYN5o7LV20g4",
        "colab_type": "code",
        "outputId": "6cfdf642-86b3-4e82-bea8-f11dd3ce14e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "train_data.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentiment_1</th>\n",
              "      <th>sentiment_2</th>\n",
              "      <th>target_words</th>\n",
              "      <th>augmented_texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@realDonaldTrump This is one of the worst time...</td>\n",
              "      <td>0</td>\n",
              "      <td>this is one of the worst times to be american ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[worst, american, serious, sure, wish, happy, ...</td>\n",
              "      <td>[this is one of the Worst times to be american...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How about the crowd in Oval in today's #AUSvIN...</td>\n",
              "      <td>1</td>\n",
              "      <td>how about the crowd in oval in today s ausvind...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[indian]</td>\n",
              "      <td>[how about the crowd in oval in today s ausvin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@skroskz @shossy2 @JoeBiden Biden &amp;amp; his so...</td>\n",
              "      <td>0</td>\n",
              "      <td>biden his son hunter took advantage of their p...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[biden, ukraine, dead]</td>\n",
              "      <td>[john_mccain his son hunter took advantage of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#etsy shop: Benedict Donald so called presiden...</td>\n",
              "      <td>1</td>\n",
              "      <td>etsy shop benedict donald so called president ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[short, streetwear]</td>\n",
              "      <td>[etsy shop benedict donald so called president...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@realDonaldTrump Good build a wall around Arka...</td>\n",
              "      <td>0</td>\n",
              "      <td>good build a wall around arkansas fucktrump fu...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[good]</td>\n",
              "      <td>[great build a wall around arkansas fucktrump ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Meanwhile ....Dhoni's Reply To ICC ......    #...</td>\n",
              "      <td>1</td>\n",
              "      <td>meanwhile dhoni s reply to icc dhonikeeptheglo...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[meanwhile, dhoni, icc]</td>\n",
              "      <td>[Meanwhile dhoni s reply to icc dhonikeepthegl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@MeredthSalenger Anything to get a war to dist...</td>\n",
              "      <td>1</td>\n",
              "      <td>anything to get a war to distract fucktrump fu...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[distract]</td>\n",
              "      <td>[anything to get a war to divert_attention fuc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Why the FUCK did Doris mention demar lmfaooooo...</td>\n",
              "      <td>0</td>\n",
              "      <td>why the fuck did doris mention demar lmfaooooo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[fuck, doris]</td>\n",
              "      <td>[why the fucking did doris mention demar lmfao...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@KimKardashian #trump2020 #fucktrump  Maybe yo...</td>\n",
              "      <td>0</td>\n",
              "      <td>trump fucktrump maybe you can hire the ex cons...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[maybe, ex, dog, porn]</td>\n",
              "      <td>[trump fucktrump probably you can hire the ex ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@matthewamiller Because there are no consequen...</td>\n",
              "      <td>0</td>\n",
              "      <td>because there are no consequences to individua...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[individual, basically, free, open, foreign, t...</td>\n",
              "      <td>[because there are no consequences to individu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>The NRA begs gun nuts for donations, spends la...</td>\n",
              "      <td>1</td>\n",
              "      <td>the nra begs gun nuts for donations spends lav...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[nra, lavishly, resist]</td>\n",
              "      <td>[the nra begs gun nuts for donations spends ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>@bbc5live @JohnnyMercerUK @Emmabarnett Imagine...</td>\n",
              "      <td>1</td>\n",
              "      <td>imagine if this was corbyn all the johnson fan...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[add, utterly, moral, high, completely, immoral]</td>\n",
              "      <td>[imagine if this was corbyn all the johnson fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>@thehill @PressSec will go down in history as ...</td>\n",
              "      <td>0</td>\n",
              "      <td>will go down in history as the liar who tried ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[liar, doctored, video, much, nail, maybe, bor...</td>\n",
              "      <td>[will go down in history as the hypocrite who ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>#Huawei asks #Verizon to pay more than $1 bill...</td>\n",
              "      <td>1</td>\n",
              "      <td>huawei asks verizon to pay more than billion f...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[verizon, clear, ride, completely, tradewar]</td>\n",
              "      <td>[huawei asks tmobile to pay more than billion ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>@ClaretSupreme @SkyFootball @SkySports Send us...</td>\n",
              "      <td>1</td>\n",
              "      <td>send us a link to a report on the vs the u s o...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[u]</td>\n",
              "      <td>[send urs a link to a report on the vs the ur ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>@BCCI It's raining tomorrow #CWC19 another was...</td>\n",
              "      <td>1</td>\n",
              "      <td>it s raining tomorrow cwc another washout sad ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Put a stick in this man's hand. Whether it be ...</td>\n",
              "      <td>0</td>\n",
              "      <td>put a stick in this man s hand whether it be d...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>@ICC Rain will take all 20 wickets.... #ShameO...</td>\n",
              "      <td>1</td>\n",
              "      <td>rain will take all wickets shameonicc</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>If the #US wanted war with #Iran, they had the...</td>\n",
              "      <td>1</td>\n",
              "      <td>if the us wanted war with iran they had their ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>[long, ago, america, ambitious, military, seve...</td>\n",
              "      <td>[if the us wanted war with iran they had their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>@JohnnyMercerUK It's not just his private life...</td>\n",
              "      <td>0</td>\n",
              "      <td>it s not just his private life he is a career ...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>[private, desperate]</td>\n",
              "      <td>[it s not just his Private life he is a career...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ...                                    augmented_texts\n",
              "0   @realDonaldTrump This is one of the worst time...  ...  [this is one of the Worst times to be american...\n",
              "1   How about the crowd in Oval in today's #AUSvIN...  ...  [how about the crowd in oval in today s ausvin...\n",
              "2   @skroskz @shossy2 @JoeBiden Biden &amp; his so...  ...  [john_mccain his son hunter took advantage of ...\n",
              "3   #etsy shop: Benedict Donald so called presiden...  ...  [etsy shop benedict donald so called president...\n",
              "4   @realDonaldTrump Good build a wall around Arka...  ...  [great build a wall around arkansas fucktrump ...\n",
              "5   Meanwhile ....Dhoni's Reply To ICC ......    #...  ...  [Meanwhile dhoni s reply to icc dhonikeepthegl...\n",
              "6   @MeredthSalenger Anything to get a war to dist...  ...  [anything to get a war to divert_attention fuc...\n",
              "7   Why the FUCK did Doris mention demar lmfaooooo...  ...  [why the fucking did doris mention demar lmfao...\n",
              "8   @KimKardashian #trump2020 #fucktrump  Maybe yo...  ...  [trump fucktrump probably you can hire the ex ...\n",
              "9   @matthewamiller Because there are no consequen...  ...  [because there are no consequences to individu...\n",
              "10  The NRA begs gun nuts for donations, spends la...  ...  [the nra begs gun nuts for donations spends ex...\n",
              "11  @bbc5live @JohnnyMercerUK @Emmabarnett Imagine...  ...  [imagine if this was corbyn all the johnson fa...\n",
              "12  @thehill @PressSec will go down in history as ...  ...  [will go down in history as the hypocrite who ...\n",
              "13  #Huawei asks #Verizon to pay more than $1 bill...  ...  [huawei asks tmobile to pay more than billion ...\n",
              "14  @ClaretSupreme @SkyFootball @SkySports Send us...  ...  [send urs a link to a report on the vs the ur ...\n",
              "15  @BCCI It's raining tomorrow #CWC19 another was...  ...                                                 []\n",
              "16  Put a stick in this man's hand. Whether it be ...  ...                                                 []\n",
              "17  @ICC Rain will take all 20 wickets.... #ShameO...  ...                                                 []\n",
              "18  If the #US wanted war with #Iran, they had the...  ...  [if the us wanted war with iran they had their...\n",
              "19  @JohnnyMercerUK It's not just his private life...  ...  [it s not just his Private life he is a career...\n",
              "\n",
              "[20 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR6hBwHv3aJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_frequent(List): \n",
        "    return max(set(List), key = List.count) \n",
        "\n",
        "def get_label(label, sent_1, sent_2):\n",
        "    if sent_1 == -1 and sent_2 == -1:\n",
        "        return label\n",
        "    elif sent_1 in [0, 1]:\n",
        "        if sent_2 in [0, 1]:\n",
        "            return most_frequent([label, sent_1, sent_2])\n",
        "        else:\n",
        "            return label\n",
        "    elif sent_2 in [0, 1]:\n",
        "        if sent_1 in [0, 1]:\n",
        "            return most_frequent([label, sent_1, sent_2])\n",
        "        else:\n",
        "            return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmnBn8KLE93t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = defaultdict()\n",
        "final_labels = defaultdict()\n",
        "\n",
        "data_counter, label_counter = 0, 0\n",
        "\n",
        "for index, row in train_data.iterrows():\n",
        "    label, sent_1, sent_2, aug_texts = row[1], row[3], row[4], row[6]\n",
        "    label = get_label(label, sent_1, sent_2)\n",
        "    for at in aug_texts:\n",
        "        final_data[data_counter] = at\n",
        "        data_counter += 1\n",
        "    for _ in range(len(aug_texts)):\n",
        "        final_labels[label_counter] = label\n",
        "        label_counter += 1\n",
        "\n",
        "\n",
        "data_df = pd.DataFrame({\"data\": final_data, \"label\": final_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adhCP5y40NJE",
        "colab_type": "text"
      },
      "source": [
        "## 4. Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHNT9CKbhZcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 10000\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "VALIDATION_SPLIT = 0.33\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "EMBEDDING_PATH = \"/gdrive/My Drive/Dataset/WordEmbeddings/glove.twitter.27B.200d.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t99h-LnFhgkw",
        "colab_type": "code",
        "outputId": "f21891e0-6293-4e40-bcd2-4d7b1b0e6346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(EMBEDDING_PATH)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1193514 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ty2vo-vTsb2",
        "colab_type": "code",
        "outputId": "94c09600-e567-4815-acf0-a068d645d59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(data_df[\"data\"])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data_df[\"data\"])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = to_categorical(np.asarray(data_df[\"label\"]))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 18543 unique tokens.\n",
            "Shape of data tensor: (27420, 200)\n",
            "Shape of label tensor: (27420, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FHsomm3hP2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsoLlR7NhbqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "    model.add(Dense(100, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(1000, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(500, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJSVkFYkhmke",
        "colab_type": "code",
        "outputId": "01ef8391-364f-4072-ee6d-dbd5c076c3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Model used : {}\".format(\"MLP Only\"))\n",
        "\n",
        "model = mlp_model()\n",
        "\n",
        "cbacks = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=100, \n",
        "          callbacks = [cbacks],\n",
        "          batch_size=256)\n",
        "\n",
        "loss, acc = model.evaluate(x_val, y_val)\n",
        "print({\"loss\": loss, \"acc\": acc})\n",
        "\n",
        "y_pred = list(map(get_indices, model.predict(x_val)))\n",
        "y_true = list(map(get_indices, y_val))\n",
        "\n",
        "print(\"F1 Score : {}\".format(f1_score(y_true=y_true, y_pred=y_pred)))\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model used : MLP Only\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 200)          3708800   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200, 100)          20100     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200, 100)          0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200, 10)           1010      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200, 10)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1000)              2001000   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               500500    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 6,282,542\n",
            "Trainable params: 2,573,742\n",
            "Non-trainable params: 3,708,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 18372 samples, validate on 9048 samples\n",
            "Epoch 1/100\n",
            "18372/18372 [==============================] - 4s 211us/step - loss: 0.6947 - acc: 0.5626 - val_loss: 0.6860 - val_acc: 0.5811\n",
            "Epoch 2/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6836 - acc: 0.5792 - val_loss: 0.6813 - val_acc: 0.5811\n",
            "Epoch 3/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.6819 - acc: 0.5798 - val_loss: 0.6804 - val_acc: 0.5811\n",
            "Epoch 4/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.6802 - acc: 0.5798 - val_loss: 0.6786 - val_acc: 0.5811\n",
            "Epoch 5/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.6762 - acc: 0.5798 - val_loss: 0.6685 - val_acc: 0.5811\n",
            "Epoch 6/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6705 - acc: 0.5798 - val_loss: 0.6663 - val_acc: 0.5811\n",
            "Epoch 7/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6621 - acc: 0.5798 - val_loss: 0.6554 - val_acc: 0.5811\n",
            "Epoch 8/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6552 - acc: 0.5797 - val_loss: 0.6428 - val_acc: 0.5811\n",
            "Epoch 9/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.6508 - acc: 0.5825 - val_loss: 0.6368 - val_acc: 0.5870\n",
            "Epoch 10/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6356 - acc: 0.6090 - val_loss: 0.5989 - val_acc: 0.7049\n",
            "Epoch 11/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.6170 - acc: 0.6306 - val_loss: 0.5795 - val_acc: 0.7392\n",
            "Epoch 12/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.6004 - acc: 0.6599 - val_loss: 0.5631 - val_acc: 0.7562\n",
            "Epoch 13/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.5845 - acc: 0.6708 - val_loss: 0.5182 - val_acc: 0.7693\n",
            "Epoch 14/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.5722 - acc: 0.6800 - val_loss: 0.5090 - val_acc: 0.7837\n",
            "Epoch 15/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.5569 - acc: 0.6890 - val_loss: 0.4869 - val_acc: 0.7943\n",
            "Epoch 16/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.5418 - acc: 0.6992 - val_loss: 0.4751 - val_acc: 0.8070\n",
            "Epoch 17/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.5353 - acc: 0.7027 - val_loss: 0.4711 - val_acc: 0.8190\n",
            "Epoch 18/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.5243 - acc: 0.7355 - val_loss: 0.4511 - val_acc: 0.8210\n",
            "Epoch 19/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.5202 - acc: 0.7522 - val_loss: 0.4513 - val_acc: 0.8282\n",
            "Epoch 20/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.5027 - acc: 0.7625 - val_loss: 0.4368 - val_acc: 0.8332\n",
            "Epoch 21/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4984 - acc: 0.7677 - val_loss: 0.4288 - val_acc: 0.8333\n",
            "Epoch 22/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4889 - acc: 0.7696 - val_loss: 0.4180 - val_acc: 0.8364\n",
            "Epoch 23/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.4807 - acc: 0.7740 - val_loss: 0.3932 - val_acc: 0.8456\n",
            "Epoch 24/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4700 - acc: 0.7824 - val_loss: 0.4008 - val_acc: 0.8452\n",
            "Epoch 25/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4644 - acc: 0.7825 - val_loss: 0.3880 - val_acc: 0.8541\n",
            "Epoch 26/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.4547 - acc: 0.7929 - val_loss: 0.3761 - val_acc: 0.8541\n",
            "Epoch 27/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4454 - acc: 0.7935 - val_loss: 0.3635 - val_acc: 0.8602\n",
            "Epoch 28/100\n",
            "18372/18372 [==============================] - 2s 95us/step - loss: 0.4436 - acc: 0.7979 - val_loss: 0.3477 - val_acc: 0.8695\n",
            "Epoch 29/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4343 - acc: 0.8044 - val_loss: 0.3526 - val_acc: 0.8651\n",
            "Epoch 30/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4300 - acc: 0.8059 - val_loss: 0.3478 - val_acc: 0.8643\n",
            "Epoch 31/100\n",
            "18372/18372 [==============================] - 2s 96us/step - loss: 0.4281 - acc: 0.8094 - val_loss: 0.3488 - val_acc: 0.8712\n",
            "9048/9048 [==============================] - 1s 90us/step\n",
            "{'loss': 0.34879850440063276, 'acc': 0.8712422847747803}\n",
            "F1 Score : 0.8919595659834926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84      3790\n",
            "           1       0.87      0.91      0.89      5258\n",
            "\n",
            "    accuracy                           0.87      9048\n",
            "   macro avg       0.87      0.86      0.87      9048\n",
            "weighted avg       0.87      0.87      0.87      9048\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVI7ZXoCkuGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation=\"relu\"))\n",
        "    model.add(Dense(100, activation=\"relu\"))\n",
        "    model.add(Dense(10, activation=\"relu\"))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGbD-sayktNN",
        "colab_type": "code",
        "outputId": "90d4ee23-22fc-4eb1-9021-9deb3460164a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "cbacks = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "print(\"Model used : {}\".format(\"LSTM Only\"))\n",
        "model = lstm_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=50, \n",
        "          callbacks = [cbacks],\n",
        "          batch_size=256)\n",
        "\n",
        "loss, acc = model.evaluate(x_val, y_val)\n",
        "print({\"loss\": loss, \"acc\": acc})\n",
        "\n",
        "y_pred = list(map(get_indices, model.predict(x_val)))\n",
        "y_true = list(map(get_indices, y_val))\n",
        "\n",
        "print(\"F1 Score : {}\".format(f1_score(y_true=y_true, y_pred=y_pred)))\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model used : LSTM Only\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 200, 200)          3708800   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200, 30)           27720     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 200, 30)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 200, 30)           7320      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200, 30)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 200, 30)           7320      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 200, 30)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1000)              6001000   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               100100    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 9,853,292\n",
            "Trainable params: 6,144,492\n",
            "Non-trainable params: 3,708,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 18372 samples, validate on 9048 samples\n",
            "Epoch 1/50\n",
            "18372/18372 [==============================] - 104s 6ms/step - loss: 0.6568 - acc: 0.6137 - val_loss: 0.6075 - val_acc: 0.6792\n",
            "Epoch 2/50\n",
            "18372/18372 [==============================] - 104s 6ms/step - loss: 0.5794 - acc: 0.7008 - val_loss: 0.5199 - val_acc: 0.7464\n",
            "Epoch 3/50\n",
            " 4864/18372 [======>.......................] - ETA: 1:13 - loss: 0.5159 - acc: 0.7420"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcr6YvYDkya4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_dimensional_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "    \n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17geDBeAk1C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbacks = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "print(\"Model used : {}\".format(\"CNN Only\"))\n",
        "model = one_dimensional_cnn()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=50,\n",
        "          callbacks = [cbacks],\n",
        "          batch_size=64)\n",
        "\n",
        "loss, acc = model.evaluate(x_val, y_val)\n",
        "print({\"loss\": loss, \"acc\": acc})\n",
        "\n",
        "y_pred = list(map(get_indices, model.predict(x_val)))\n",
        "y_true = list(map(get_indices, y_val))\n",
        "\n",
        "print(\"F1 Score : {}\".format(f1_score(y_true=y_true, y_pred=y_pred)))\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnHm0htuj2fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "labels = [0, 1]\n",
        "title='Confusion matrix'\n",
        "print(cm)\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title(title)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(labels))\n",
        "plt.xticks(tick_marks, labels, rotation=45)\n",
        "plt.yticks(tick_marks, labels)\n",
        "fmt = 'd'\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], fmt),\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqVF3dyVk4VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(Conv1D(128, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(Conv1D(64, 4, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(30, activation=\"relu\", return_sequences=True, recurrent_dropout=0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7lKTaJUk6st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbacks = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "print(\"Model used : {}\".format(\"CNN + LSTM Only\"))\n",
        "model = cnn_lstm()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=50, \n",
        "          callbacks = [cbacks],\n",
        "          batch_size=256)\n",
        "\n",
        "loss, acc = model.evaluate(x_val, y_val)\n",
        "print({\"loss\": loss, \"acc\": acc})\n",
        "\n",
        "y_pred = list(map(get_indices, model.predict(x_val)))\n",
        "y_true = list(map(get_indices, y_val))\n",
        "\n",
        "print(\"F1 Score : {}\".format(f1_score(y_true=y_true, y_pred=y_pred)))\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJmOBEklytq",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY1gPAzel0-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(line):\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "    combined_pat = r'|'.join((pat1, pat2))\n",
        "    \n",
        "    soup = BeautifulSoup(line, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    split = lower_case.split(\" \")\n",
        "    return \" \".join(list(filter(None, split))).strip()\n",
        "\n",
        "def get_indices(x): return np.argmax(x)\n",
        "\n",
        "test_data[\"clean_text\"] = train_data[\"text\"].apply(lambda x: clean_text(x))\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bzf89_plxsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(test_data[\"clean_text\"])\n",
        "sequences = tokenizer.texts_to_sequences(test_data[\"clean_text\"])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PczE9N5mWN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt8FmoIbmgC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(data)\n",
        "\n",
        "y_pred = list(map(get_indices, preds))\n",
        "pd.DataFrame({\"label\": y_pred}).to_csv(\"/gdrive/My Drive/Dataset/AICrowd/HateSpeechDetection/submission_satwik.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}